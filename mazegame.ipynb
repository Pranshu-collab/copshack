{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "499da7fb-da0c-4dbc-aee4-82d4e1a76cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41be1da6-8f12-45bc-b519-2ae4bb867880",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze3(gym.Env):\n",
    "    def __init__(self, maze):\n",
    "        super(Maze3, self).__init__()\n",
    "        self.maze = np.array(maze)  # Maze represented as a 2D numpy array\n",
    "        self.start_pos = np.where((self.maze == 'S')[0])  # Starting position\n",
    "        self.goal_pos = np.where((self.maze == 'G')[0])  # Goal position\n",
    "        self.current_pos = self.start_pos[0] #starting position is current posiiton of agent\n",
    "        self.num_rows, self.num_cols = self.maze.shape\n",
    "\n",
    "        # 4 possible actions: 0=up, 1=down, 2=left, 3=right\n",
    "        self.action_space = spaces.Discrete(4)  \n",
    "\n",
    "        # Observation space is grid of size:rows x columns\n",
    "        self.observation_space = spaces.Tuple((spaces.Discrete(self.num_rows), spaces.Discrete(self.num_cols)))\n",
    "\n",
    "        # Initialize Pygame\n",
    "        pygame.init()\n",
    "        self.cell_size = 100\n",
    "\n",
    "        # setting display size\n",
    "        self.screen = pygame.display.set_mode((self.num_cols * self.cell_size, self.num_rows * self.cell_size))\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_pos = self.start_pos[0]\n",
    "        return tuple(self.current_pos)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Move the agent based on the selected action\n",
    "        new_pos = np.array(self.current_pos)\n",
    "        if action == 0:  # Up\n",
    "            new_pos[0] -= 1\n",
    "        elif action == 1:  # Down\n",
    "            new_pos[0] += 1\n",
    "        elif action == 2:  # Left\n",
    "            new_pos[0] -= 1\n",
    "        elif action == 3:  # Right\n",
    "            new_pos[0] += 1\n",
    "            \n",
    "\n",
    "        # Check if the new position is valid\n",
    "        if self._is_valid_position(new_pos):\n",
    "            self.current_pos = tuple(new_pos)\n",
    "\n",
    "        # Reward function\n",
    "        if np.array_equal(self.current_pos, self.goal_pos):\n",
    "            reward = 1.0\n",
    "            done = True\n",
    "        else:\n",
    "            reward = 0.0\n",
    "            done = False\n",
    "\n",
    "        return tuple(self.current_pos), reward, done, {}\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def _is_valid_position(self, pos):\n",
    "        row, col = pos\n",
    "   \n",
    "        # If agent goes out of the grid\n",
    "        if row < 0 or col < 0 or row >= self.num_rows or col >= self.num_cols:\n",
    "            return False\n",
    "\n",
    "        # If the agent hits an obstacle\n",
    "        if self.maze[row, col] == '#':\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def render(self):\n",
    "        # Clear the screen\n",
    "        self.screen.fill((255, 255, 255))  \n",
    "\n",
    "        # Draw env elements one cell at a time\n",
    "        for row in range(self.num_rows):\n",
    "            for col in range(self.num_cols):\n",
    "                cell_left = col * self.cell_size\n",
    "                cell_top = row * self.cell_size\n",
    "            \n",
    "                try:\n",
    "                    print(np.array(self.current_pos)==np.array([row,col]).reshape(-1,1))\n",
    "                except Exception as e:\n",
    "                    print('Initial state')\n",
    "\n",
    "                if self.maze[row, col] == '#':  # Obstacle\n",
    "                    pygame.draw.rect(self.screen, (0, 0, 0), (cell_left, cell_top, self.cell_size, self.cell_size))\n",
    "                elif self.maze[row, col] == 'S':  # Starting position\n",
    "                    pygame.draw.rect(self.screen, (0, 255, 0), (cell_left, cell_top, self.cell_size, self.cell_size))\n",
    "                elif self.maze[row, col] == 'G':  # Goal position\n",
    "                    pygame.draw.rect(self.screen, (255, 0, 0), (cell_left, cell_top, self.cell_size, self.cell_size))\n",
    "\n",
    "                if np.array_equal(np.array(self.current_pos), np.array([row, col]).reshape(-1,1)):  # Agent position\n",
    "                    pygame.draw.rect(self.screen, (0, 0, 255), (cell_left, cell_top, self.cell_size, self.cell_size))\n",
    "\n",
    "        pygame.display.update()  # Update the display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bfe9039-aa93-4868-ad4d-4763eea0e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "# Register the environment\n",
    "gym.envs.registration.register(\n",
    "    id='Mazegame1-v0',\n",
    "    entry_point='__main__:Maze3', \n",
    "    kwargs={'maze': None} \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63ccf03e-f3fd-4806-b34d-bb2916e033a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pranshu\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\Pranshu\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\Pranshu\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:199: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` should be `(obs, info)` by default, , where `obs` is a observation and `info` is a dictionary containing additional information.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True]\n",
      " [ True]]\n",
      "[[ True]\n",
      " [False]]\n",
      "[[ True]\n",
      " [False]]\n",
      "[[ True]\n",
      " [False]]\n",
      "[[ True]\n",
      " [False]]\n",
      "[[ True]\n",
      " [False]]\n",
      "[[ True]\n",
      " [False]]\n",
      "[[ True]\n",
      " [False]]\n",
      "[[False]\n",
      " [ True]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [ True]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [ True]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [ True]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [ True]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [ True]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [ True]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n",
      "[[False]\n",
      " [False]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mget()\n\u001b[0;32m     20\u001b[0m action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()  \u001b[38;5;66;03m# Random action selection\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m obs, reward, done, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     22\u001b[0m env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReward:\u001b[39m\u001b[38;5;124m'\u001b[39m, reward)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gym\\wrappers\\env_checker.py:37\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:214\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[1;34m(env, action)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;66;03m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m result \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    216\u001b[0m     result, \u001b[38;5;28mtuple\u001b[39m\n\u001b[0;32m    217\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpects step result to be a tuple, actual type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "Cell \u001b[1;32mIn[2], line 41\u001b[0m, in \u001b[0;36mMaze3.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     new_pos[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Check if the new position is valid\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_valid_position(new_pos):\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(new_pos)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Reward function\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 58\u001b[0m, in \u001b[0;36mMaze3._is_valid_position\u001b[1;34m(self, pos)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_valid_position\u001b[39m(\u001b[38;5;28mself\u001b[39m, pos):\n\u001b[1;32m---> 58\u001b[0m     row, col \u001b[38;5;241m=\u001b[39m pos\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# If agent goes out of the grid\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m row \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m col \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m row \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;129;01mor\u001b[39;00m col \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_cols:\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "maze = [\n",
    "    ['S','.','.','.','#','.','.','.'],\n",
    "    ['.','#','.','#','.','#','.','#'],\n",
    "    ['.','.','.','.','.','.','.','.'],\n",
    "    ['#','.','#','.','#','.','#','.'],\n",
    "    ['.','.','.','#','.','.','.','.'],\n",
    "    ['.','#','.','.','.','.','#','.'],\n",
    "    ['#','.','.','.','#','.','.','.'],\n",
    "    ['.','#','.','.','#','.','#','G']\n",
    "    \n",
    "]\n",
    "# Test the environment\n",
    "env = gym.make('Mazegame1-v0',maze=maze)\n",
    "obs = env.reset()\n",
    "env.render()\n",
    "\n",
    "done = False\n",
    "while True:\n",
    "    pygame.event.get()\n",
    "    action = env.action_space.sample()  # Random action selection\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    print('Reward:', reward)\n",
    "    print('Done:', done)\n",
    "\n",
    "    pygame.time.wait(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e703225f-ccf6-4c2f-b483-40a7f1506a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pranshu\\AppData\\Local\\Temp\\ipykernel_27308\\2916780491.py:5: DeprecationWarning: Calling nonzero on 0d arrays is deprecated, as it behaves surprisingly. Use `atleast_1d(cond).nonzero()` if the old behavior was intended. If the context of this warning is of the form `arr[nonzero(cond)]`, just use `arr[cond]`.\n",
      "  self.start_pos = np.where(self.maze == 'S')  # Starting position\n",
      "C:\\Users\\Pranshu\\AppData\\Local\\Temp\\ipykernel_27308\\2916780491.py:6: DeprecationWarning: Calling nonzero on 0d arrays is deprecated, as it behaves surprisingly. Use `atleast_1d(cond).nonzero()` if the old behavior was intended. If the context of this warning is of the form `arr[nonzero(cond)]`, just use `arr[cond]`.\n",
      "  self.goal_pos = np.where(self.maze == 'G')  # Goal position\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 46\u001b[0m\n\u001b[0;32m     42\u001b[0m             epsilon \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m epsilon_decay\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 46\u001b[0m     env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMazegame1-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m     MAX_EPISODES \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m9999\u001b[39m\n\u001b[0;32m     48\u001b[0m     MAX_TRY \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gym\\envs\\registration.py:640\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    637\u001b[0m     render_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 640\u001b[0m     env \u001b[38;5;241m=\u001b[39m env_creator(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    642\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    643\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot an unexpected keyword argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrender_mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    644\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m apply_human_rendering\n\u001b[0;32m    645\u001b[0m     ):\n",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m, in \u001b[0;36mMaze3.__init__\u001b[1;34m(self, maze)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoal_pos \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaze \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mG\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Goal position\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_pos[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m#starting position is current posiiton of agent\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_rows, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaze\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 4 possible actions: 0=up, 1=down, 2=left, 3=right\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space \u001b[38;5;241m=\u001b[39m spaces\u001b[38;5;241m.\u001b[39mDiscrete(\u001b[38;5;241m4\u001b[39m)  \n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "def simulate():\n",
    "    global epsilon, epsilon_decay\n",
    "    for episode in range(MAX_EPISODES):\n",
    "\n",
    "        # Init environment\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "\n",
    "        # AI tries up to MAX_TRY times\n",
    "        for t in range(MAX_TRY):\n",
    "\n",
    "            # In the beginning, do random action to learn\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(q_table[state])\n",
    "\n",
    "            # Do action and get result\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Get correspond q value from state, action pair\n",
    "            q_value = q_table[state][action]\n",
    "            best_q = np.max(q_table[next_state])\n",
    "\n",
    "        \n",
    "            q_table[state][action] = (1 - learning_rate) * q_value + learning_rate * (reward + gamma * best_q)\n",
    "\n",
    "            # Set up for the next iteration\n",
    "            state = next_state\n",
    "\n",
    "            # Draw games\n",
    "            env.render()\n",
    "\n",
    "            # When episode is done, print reward\n",
    "            if done or t >= MAX_TRY - 1:\n",
    "                print(\"Episode %d finished after %i time steps with total reward = %f.\" % (episode, t, total_reward))\n",
    "                break\n",
    "\n",
    "        # exploring rate decay\n",
    "        if epsilon >= 0.005:\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"Mazegame1-v0\")\n",
    "    MAX_EPISODES = 9999\n",
    "    MAX_TRY = 1000\n",
    "    epsilon = 1\n",
    "    epsilon_decay = 0.999\n",
    "    learning_rate = 0.1\n",
    "    gamma = 0.6\n",
    "    num_box = tuple((env.observation_space.high + np.ones(env.observation_space.shape)).astype(int))\n",
    "    q_table = np.zeros(num_box + (env.action_space.n,))\n",
    "    simulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5db13ac-775e-49d6-b480-5b15796eb0be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3f6cc-a3f0-4c50-8a38-4184cc37c8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
